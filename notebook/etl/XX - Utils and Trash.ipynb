{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XX - Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. wiki API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")\n",
    "\n",
    "p_wiki = wiki_wiki.page(\"Outline_of_academic_disciplines\")\n",
    "print(p_wiki.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Recursive Approach\n",
    "Start at first 'nw-headline' header:\n",
    " - if next sibling != sub headers (h3, h4) or (ul, li): terminate recursion\n",
    " - store (name, ref) tuple, go deeper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRASH RECURSIVE APPROACH\n",
    "# wikiPagePathDF = pd.DataFrame(\"wikiPagePath\", \"category_h2_1 sub_category_h3_2, topic_3, subtopic_4, subtopic_5, page\")\n",
    "\n",
    "results = []\n",
    "\n",
    "#1 - iterate over h2 (category) headings, clean, write, and recurse levels\n",
    "for h2 in soup.find_all(\"h2\"):\n",
    "    h2_span = h2.find('span', class_='mw-headline')\n",
    "    \n",
    "    #1.1 clean and write h2s\n",
    "    if h2_span != None and h2_span.text not in ['See also', 'References', 'External links']: #skip non-category headers\n",
    "#         print(h2_span.text)\n",
    "#         a = h2_span.find('a')\n",
    "#         if a != None:\n",
    "#             href = a['href']\n",
    "#         else:\n",
    "#         # grab article ref from matching main article\n",
    "#             candidate_articles = h2_span.find_next('div').find_all('a')\n",
    "#             for article in candidate_articles:\n",
    "#                 href = article['href'] \n",
    "#             href = get_best_match(h2_span.text, candidate_articles) \n",
    "#         print(h2_span.text,href)\n",
    "        \n",
    "        #2 - iterate over h3 groups (children of h2)\n",
    "        \n",
    "        #2.1 - collect h3 groups\n",
    "        h2_children = []\n",
    "        for tag in h2_span.parent.next_siblings:\n",
    "            if tag.name != 'h2':\n",
    "                h2_children.append(tag)\n",
    "            else:\n",
    "                break\n",
    "           \n",
    "        #2.2 - get and process h3 group\n",
    "        h2_children\n",
    "        break\n",
    "#         print(h2_children)\n",
    "#     for \n",
    "#         results.append(wikiPagePath(h2_span.text, 'NA','NA','NA','NA', href))\n",
    "# print(results)\n",
    "\n",
    "#         break\n",
    "#         break\n",
    "#         for li in .find('li'):\n",
    "# #             subtopic_4 = li.text\n",
    "#             print(li)\n",
    "#             break\n",
    "            \n",
    "            \n",
    "        \n",
    "#         print(el.find('ul').find('ul'), 'STOP')\n",
    "#         for li in el.find('ul').find('li'):\n",
    "#             print(li.find('a'))\n",
    "#         break\n",
    "        #unpack topics up to 3 levels\n",
    "#         while\n",
    "    \n",
    "        #TODO - clear topics metadata when nested bulleted list unpacking completed\n",
    "#         subtopic_4 = subtopic_5 = '' \n",
    "#start at first category (h2)\n",
    "start_of_outline = soup.select(\"h2 span\", class_='mw-headline')[0]\n",
    "category_h2_1 = start_of_outline.text\n",
    "page = start_of_outline.a['href']\n",
    "sub_category_h3_2 = topic_3 = subtopic_4 = subtopic_5 = ''\n",
    "\n",
    "print(category_h2_1, sub_category_h3_2, topic_3, subtopic_4, subtopic_5, page)\n",
    "\n",
    "for el in start_of_outline.parent.next_siblings:\n",
    "    if el.name == 'h2':\n",
    "        h2_span = el.find('span', class_='mw-headline')\n",
    "        if h2_span != None and h2_span.text not in ['See also', 'References', 'External links']: #skip non-category headers\n",
    "            category_h2_1 = h2_span.text\n",
    "            a = h2_span.find('a')\n",
    "            if a != None:\n",
    "                page_href = a['href']\n",
    "            else:\n",
    "            # grab article ref from matching main article\n",
    "                candidate_articles = h2_span.find_next('div').find_all('a')\n",
    "                page_href = [article['href'] for article in candidate_articles]\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        sub_category_h3_2 = topic_3 = subtopic_4 = subtopic_5 = ''\n",
    "            \n",
    "    elif el.name == 'h3':\n",
    "        h3_span = el.find('span', class_='mw-headline')\n",
    "        sub_category_h3_2 = h3_span.text\n",
    "        a = h3_span.find('a')\n",
    "        if a != None:\n",
    "            page_href = a['href']\n",
    "        else:\n",
    "        # grab article ref from matching main article\n",
    "            candidate_articles = h3_span.find_next('div').find_all('a')\n",
    "            page_href = [article['href'] for article in candidate_articles]\n",
    "        \n",
    "        topic_3 = subtopic_4 = subtopic_5 = ''\n",
    "            \n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    print(category_h2_1, sub_category_h3_2, topic_3, subtopic_4, subtopic_5, page_href)\n",
    "    \n",
    "    \n",
    "#     if h4:\n",
    "#         topic_3\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
